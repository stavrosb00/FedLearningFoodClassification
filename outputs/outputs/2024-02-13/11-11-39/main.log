[2024-02-13 11:11:40,666][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)
[2024-02-13 11:11:47,199][flwr][INFO] - Flower VCE: Ray initialized with resources: {'memory': 1008402432.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 504201216.0, 'node:__internal_head__': 1.0, 'CPU': 4.0}
[2024-02-13 11:11:47,200][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2024-02-13 11:11:47,201][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.0}
[2024-02-13 11:11:47,215][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[2024-02-13 11:11:47,216][flwr][INFO] - Initializing global parameters
[2024-02-13 11:11:47,217][flwr][INFO] - Requesting initial parameters from one random client
[2024-02-13 11:11:55,657][flwr][INFO] - Received initial parameters from one random client
[2024-02-13 11:11:55,659][flwr][INFO] - Evaluating initial parameters
[2024-02-13 11:11:57,308][flwr][ERROR] - stack expects each tensor to be equal size, but got [3, 512, 512] at entry 0 and [3, 511, 512] at entry 6
[2024-02-13 11:11:57,314][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\app.py", line 308, in start_simulation
    hist = run_fl(
           ^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\server\app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\server\server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\server\strategy\fedavg.py", line 165, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\server.py", line 53, in evaluate_fn
    loss, accuracy = test(model, testloader, device)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\model.py", line 127, in test
    for data in testloader:
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\utils\data\dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\utils\data\dataloader.py", line 674, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 54, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\utils\data\_utils\collate.py", line 265, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\utils\data\_utils\collate.py", line 142, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\utils\data\_utils\collate.py", line 142, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\utils\data\_utils\collate.py", line 119, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\utils\data\_utils\collate.py", line 162, in collate_tensor_fn
    return torch.stack(batch, 0, out=out)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: stack expects each tensor to be equal size, but got [3, 512, 512] at entry 0 and [3, 511, 512] at entry 6

[2024-02-13 11:11:57,326][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 0.0} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 0.0}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
