[2024-02-18 13:47:37,200][flwr][WARNING] - Both server and strategy were provided, ignoring strategy
[2024-02-18 13:47:37,202][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)
[2024-02-18 13:47:44,549][flwr][INFO] - Flower VCE: Ray initialized with resources: {'CPU': 4.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 908793446.0, 'memory': 1817586894.0, 'node:__internal_head__': 1.0}
[2024-02-18 13:47:44,550][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2024-02-18 13:47:44,550][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.0}
[2024-02-18 13:47:44,572][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[2024-02-18 13:47:44,573][flwr][INFO] - Initializing global parameters
[2024-02-18 13:47:44,574][flwr][INFO] - Requesting initial parameters from one random client
[2024-02-18 13:47:53,624][flwr][INFO] - Received initial parameters from one random client
[2024-02-18 13:47:53,664][flwr][INFO] - Evaluating initial parameters
[2024-02-18 13:51:00,599][flwr][INFO] - initial parameters (loss, other metrics): 0.023628299474716185, {'accuracy': 24.55078125}
[2024-02-18 13:51:00,616][flwr][INFO] - FL starting
[2024-02-18 13:51:01,611][flwr][DEBUG] - fit_round 1: strategy sampled 10 clients (out of 100)
[2024-02-18 13:51:56,746][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=252, ip=127.0.0.1, actor_id=83e5b1701c3403c55a15c8cd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000131EC38A090>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\client_scaffold.py", line 113, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\model.py", line 160, in train_scaffold
    optimizer.step(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\optim\optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\model.py", line 134, in step
    p.data.add_(other=(p.grad.data + sc - cc), alpha=-group['lr'])
                       ~~~~~~~~~~~~^~~~
RuntimeError: The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 3

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=252, ip=127.0.0.1, actor_id=83e5b1701c3403c55a15c8cd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000131EC38A090>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 60 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\client_scaffold.py", line 113, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\model.py", line 160, in train_scaffold\n    optimizer.step(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\torch\\optim\\optimizer.py", line 373, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\model.py", line 134, in step\n    p.data.add_(other=(p.grad.data + sc - cc), alpha=-group[\'lr\'])\n                       ~~~~~~~~~~~~^~~~\nRuntimeError: The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 3\n',)

[2024-02-18 13:51:56,823][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=252, ip=127.0.0.1, actor_id=83e5b1701c3403c55a15c8cd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000131EC38A090>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\client_scaffold.py", line 113, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\model.py", line 160, in train_scaffold
    optimizer.step(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\optim\optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\model.py", line 134, in step
    p.data.add_(other=(p.grad.data + sc - cc), alpha=-group['lr'])
                       ~~~~~~~~~~~~^~~~
RuntimeError: The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 3

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=252, ip=127.0.0.1, actor_id=83e5b1701c3403c55a15c8cd01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x00000131EC38A090>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 60 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\client_scaffold.py", line 113, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\model.py", line 160, in train_scaffold\n    optimizer.step(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\torch\\optim\\optimizer.py", line 373, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\model.py", line 134, in step\n    p.data.add_(other=(p.grad.data + sc - cc), alpha=-group[\'lr\'])\n                       ~~~~~~~~~~~~^~~~\nRuntimeError: The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 3\n',)
[2024-02-18 13:51:56,905][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=4080, ip=127.0.0.1, actor_id=a75b05a692ea7650bbe95d7101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x000002492F839690>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\client_scaffold.py", line 113, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\model.py", line 160, in train_scaffold
    optimizer.step(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\optim\optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\model.py", line 134, in step
    p.data.add_(other=(p.grad.data + sc - cc), alpha=-group['lr'])
                       ~~~~~~~~~~~~^~~~
RuntimeError: The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 3

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=4080, ip=127.0.0.1, actor_id=a75b05a692ea7650bbe95d7101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x000002492F839690>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 73 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\client_scaffold.py", line 113, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\model.py", line 160, in train_scaffold\n    optimizer.step(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\torch\\optim\\optimizer.py", line 373, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\model.py", line 134, in step\n    p.data.add_(other=(p.grad.data + sc - cc), alpha=-group[\'lr\'])\n                       ~~~~~~~~~~~~^~~~\nRuntimeError: The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 3\n',)

[2024-02-18 13:51:56,925][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=4080, ip=127.0.0.1, actor_id=a75b05a692ea7650bbe95d7101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x000002492F839690>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\client_scaffold.py", line 113, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\model.py", line 160, in train_scaffold
    optimizer.step(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\torch\optim\optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\Desktop\PythonProjects\FlowerYtTut\model.py", line 134, in step
    p.data.add_(other=(p.grad.data + sc - cc), alpha=-group['lr'])
                       ~~~~~~~~~~~~^~~~
RuntimeError: The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 3

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=4080, ip=127.0.0.1, actor_id=a75b05a692ea7650bbe95d7101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x000002492F839690>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 73 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\client_scaffold.py", line 113, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\model.py", line 160, in train_scaffold\n    optimizer.step(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\Stavros\\miniconda3\\envs\\pyt_pg\\Lib\\site-packages\\torch\\optim\\optimizer.py", line 373, in wrapper\n    out = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\Stavros\\Desktop\\PythonProjects\\FlowerYtTut\\model.py", line 134, in step\n    p.data.add_(other=(p.grad.data + sc - cc), alpha=-group[\'lr\'])\n                       ~~~~~~~~~~~~^~~~\nRuntimeError: The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 3\n',)
[2024-02-18 13:52:45,920][flwr][ERROR] - The actor died unexpectedly before finishing this task.
[2024-02-18 13:52:45,941][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 418, in get_client_result
    self.process_unordered_future(timeout=timeout)
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 400, in process_unordered_future
    self._return_actor(actor)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\util\actor_pool.py", line 366, in _return_actor
    self.submit(*self._pending_submits.pop(0))
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 249, in submit
    future = fn(actor, client_fn, job_fn, cid, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 148, in <lambda>
    lambda a, c_fn, j_fn, cid, state: a.run.remote(c_fn, j_fn, cid, state),
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 144, in remote
    return self._remote(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 423, in _start_span
    return method(self, args, kwargs, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 190, in _remote
    return invocation(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 177, in invocation
    return actor._actor_method_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 1175, in _actor_method_call
    object_refs = worker.core_worker.submit_actor_task(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "python\ray\_raylet.pyx", line 3350, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 3355, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 649, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 640, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 734, in ray._raylet.prepare_args_internal
  File "python\ray\_raylet.pyx", line 2939, in ray._raylet.CoreWorker.put_serialized_object_and_increment_local_ref
  File "python\ray\_raylet.pyx", line 2831, in ray._raylet.CoreWorker._create_put_buffer
  File "python\ray\_raylet.pyx", line 412, in ray._raylet.check_status
ray.exceptions.RaySystemError: System error: Unknown error

[2024-02-18 13:52:45,944][flwr][ERROR] - The actor died unexpectedly before finishing this task.
[2024-02-18 13:52:45,948][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

[2024-02-18 13:52:45,948][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

[2024-02-18 13:52:45,951][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

[2024-02-18 13:52:45,952][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\worker.py", line 2513, in get
    raise ValueError(
ValueError: 'object_refs' must either be an ObjectRef or a list of ObjectRefs.

[2024-02-18 13:52:45,954][flwr][ERROR] - System error: Unknown error
[2024-02-18 13:52:45,955][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 418, in get_client_result
    self.process_unordered_future(timeout=timeout)
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 400, in process_unordered_future
    self._return_actor(actor)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\util\actor_pool.py", line 366, in _return_actor
    self.submit(*self._pending_submits.pop(0))
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 249, in submit
    future = fn(actor, client_fn, job_fn, cid, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 148, in <lambda>
    lambda a, c_fn, j_fn, cid, state: a.run.remote(c_fn, j_fn, cid, state),
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 144, in remote
    return self._remote(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 423, in _start_span
    return method(self, args, kwargs, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 190, in _remote
    return invocation(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 177, in invocation
    return actor._actor_method_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 1175, in _actor_method_call
    object_refs = worker.core_worker.submit_actor_task(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "python\ray\_raylet.pyx", line 3350, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 3355, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 649, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 640, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 734, in ray._raylet.prepare_args_internal
  File "python\ray\_raylet.pyx", line 2939, in ray._raylet.CoreWorker.put_serialized_object_and_increment_local_ref
  File "python\ray\_raylet.pyx", line 2831, in ray._raylet.CoreWorker._create_put_buffer
  File "python\ray\_raylet.pyx", line 412, in ray._raylet.check_status
ray.exceptions.RaySystemError: System error: Unknown error

[2024-02-18 13:52:45,961][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 315, in _fetch_future_result
    raise ex
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\worker.py", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.

[2024-02-18 13:52:45,962][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 315, in _fetch_future_result
    raise ex
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\worker.py", line 2526, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.

[2024-02-18 13:52:45,962][flwr][ERROR] - 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
[2024-02-18 13:52:45,963][flwr][ERROR] - 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
[2024-02-18 13:52:45,963][flwr][ERROR] - 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
[2024-02-18 13:52:45,965][flwr][ERROR] - 'object_refs' must either be an ObjectRef or a list of ObjectRefs.
[2024-02-18 13:52:45,967][flwr][ERROR] - System error: Unknown error
[2024-02-18 13:52:45,968][flwr][ERROR] - The actor died unexpectedly before finishing this task.
[2024-02-18 13:52:45,968][flwr][ERROR] - The actor died unexpectedly before finishing this task.
