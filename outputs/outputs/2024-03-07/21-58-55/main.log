[2024-03-07 21:58:57,053][flwr][WARNING] - Both server and strategy were provided, ignoring strategy
[2024-03-07 21:58:57,053][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
[2024-03-07 21:59:01,885][flwr][INFO] - Flower VCE: Ray initialized with resources: {'memory': 19277293979.0, 'CPU': 12.0, 'object_store_memory': 9638646988.0, 'node:__internal_head__': 1.0, 'accelerator_type:G': 1.0, 'node:155.207.19.230': 1.0, 'GPU': 2.0}
[2024-03-07 21:59:01,886][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2024-03-07 21:59:01,886][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.5}
[2024-03-07 21:59:01,909][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
[2024-03-07 21:59:01,910][flwr][INFO] - Initializing global parameters
[2024-03-07 21:59:01,910][flwr][INFO] - Requesting initial parameters from one random client
[2024-03-07 21:59:05,723][flwr][INFO] - Received initial parameters from one random client
[2024-03-07 21:59:05,724][flwr][INFO] - Evaluating initial parameters
[2024-03-07 21:59:15,476][flwr][INFO] - initial parameters (loss, other metrics): 0.023086071193218233, {'accuracy': 27.63671875}
[2024-03-07 21:59:15,476][flwr][INFO] - FL starting
[2024-03-07 21:59:15,477][flwr][DEBUG] - fit_round 1: strategy sampled 8 clients (out of 16)
[2024-03-07 21:59:34,166][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 21:59:34,167][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 21:59:34,648][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 217.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 15 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 217.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 21:59:34,648][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 217.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 15 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 217.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 21:59:35,316][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 127.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 13 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 127.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 21:59:35,317][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 127.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 13 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 127.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 21:59:39,853][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 11 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 21:59:39,854][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 11 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 21:59:40,247][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 127.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 6 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 127.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 21:59:40,247][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 127.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 6 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 127.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 21:59:40,675][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 127.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 127.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 21:59:40,676][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 127.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 127.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 21:59:40,677][flwr][DEBUG] - fit_round 1 received 2 results and 6 failures
[2024-03-07 21:59:48,059][flwr][INFO] - fit progress: (1, 0.021947802603244783, {'accuracy': 28.30078125}, 32.58229861903237)
[2024-03-07 21:59:48,059][flwr][DEBUG] - evaluate_round 1: strategy sampled 8 clients (out of 16)
[2024-03-07 21:59:52,682][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 7 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 21:59:52,683][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 7 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 21:59:55,360][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 4 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 21:59:55,361][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 4 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 21:59:55,425][flwr][DEBUG] - evaluate_round 1 received 6 results and 2 failures
[2024-03-07 21:59:55,426][flwr][DEBUG] - fit_round 2: strategy sampled 8 clients (out of 16)
[2024-03-07 22:00:01,504][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 5 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 22:00:01,504][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 5 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 22:00:02,555][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 109.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 3 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 109.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:00:02,572][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 109.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 3 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 109.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:00:02,572][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 6 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:00:02,609][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 6 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:00:07,659][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 9 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 22:00:07,660][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 9 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 22:00:07,997][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 111.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 111.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:00:07,998][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 111.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 111.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:00:08,378][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 11 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:00:08,379][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 11 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:00:09,145][flwr][DEBUG] - fit_round 2 received 2 results and 6 failures
[2024-03-07 22:00:16,065][flwr][INFO] - fit progress: (2, 0.02080556857585907, {'accuracy': 45.21484375}, 60.589051395014394)
[2024-03-07 22:00:16,066][flwr][DEBUG] - evaluate_round 2: strategy sampled 8 clients (out of 16)
[2024-03-07 22:00:20,498][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 3 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 22:00:20,499][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 3 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 22:00:23,296][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 13 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 22:00:23,297][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 13 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 22:00:23,298][flwr][DEBUG] - evaluate_round 2 received 6 results and 2 failures
[2024-03-07 22:00:23,299][flwr][DEBUG] - fit_round 3: strategy sampled 8 clients (out of 16)
[2024-03-07 22:00:29,541][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 22:00:29,557][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 22:00:29,716][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 109.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 8 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 109.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:00:29,716][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 109.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 8 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 109.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:00:30,308][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 6 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:00:30,308][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 6 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:00:35,251][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 111.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 111.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:00:35,251][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward
    out = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 111.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655557, ip=155.207.19.230, actor_id=2276e2a02f29d3d832f60ae601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f3872752d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 92, in forward\n    out = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 10.75 GiB total capacity; 2.98 GiB already allocated; 111.62 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:00:35,716][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 4 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:00:35,717][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl
    x = self.bn1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655556, ip=155.207.19.230, actor_id=78e7e676441f4117e732a4f601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f352514acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 4 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 269, in _forward_impl\n    x = self.bn1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 10.75 GiB total capacity; 1.23 GiB already allocated; 111.62 MiB free; 1.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:00:35,780][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 14 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 22:00:35,781][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 14 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 22:00:36,976][flwr][DEBUG] - fit_round 3 received 2 results and 6 failures
[2024-03-07 22:00:43,978][flwr][INFO] - fit progress: (3, 0.019993782460689544, {'accuracy': 48.76953125}, 88.50167121202685)
[2024-03-07 22:00:43,979][flwr][DEBUG] - evaluate_round 3: strategy sampled 8 clients (out of 16)
[2024-03-07 22:00:47,991][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 22:00:47,991][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 22:00:51,123][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 15 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)

[2024-03-07 22:00:51,123][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate
    return maybe_call_evaluate(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate
    return client.evaluate(evaluate_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate
    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate
    loss, accuracy = test(self.model, self.valloader, self.device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl
    x = self.conv1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Unable to find a valid cuDNN algorithm to run convolution

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=655558, ip=155.207.19.230, actor_id=c6f04fa3c1b50a9e17cb061e01000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f52df946d10>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 15 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 225, in evaluate\n    return maybe_call_evaluate(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 254, in maybe_call_evaluate\n    return client.evaluate(evaluate_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 262, in _evaluate\n    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 96, in evaluate\n    loss, accuracy = test(self.model, self.valloader, self.device)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 206, in test\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 268, in _forward_impl\n    x = self.conv1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n',)
[2024-03-07 22:00:51,377][flwr][DEBUG] - evaluate_round 3 received 6 results and 2 failures
[2024-03-07 22:00:51,378][flwr][INFO] - FL finished in 95.9013216610183
[2024-03-07 22:00:51,379][flwr][INFO] - app_fit: losses_distributed [(1, 0.07330776557878212), (2, 0.0710845998039952), (3, 0.0649973827379721)]
[2024-03-07 22:00:51,380][flwr][INFO] - app_fit: metrics_distributed_fit {'loss': [(1, 1.4601597587267559), (2, 1.2923919359842937), (3, 1.1878272692362468)], 'accuracy': [(1, 22.724780559539795), (2, 39.3640349706014), (3, 50.32894706726074)], 'mean_diff_acc': [(1, 0.0), (2, 0.0), (3, 0.0)], 'var_diff_acc': [(1, 30.05732432186658), (2, 3.077870055175481), (3, 6.92520511447583)]}
[2024-03-07 22:00:51,380][flwr][INFO] - app_fit: metrics_distributed {'loss': [(1, 0.07330776557878212), (2, 0.0710845998039952), (3, 0.0649973827379721)], 'accuracy': [(1, 26.62036959330241), (2, 44.444442431131996), (3, 55.55555407206217)], 'mean_diff_acc': [(1, -1.1842378929335002e-15), (2, 2.3684757858670005e-15), (3, 2.3684757858670005e-15)], 'var_diff_acc': [(1, 161.0189506873511), (2, 144.03292141824255), (3, 154.32098294481898)]}
[2024-03-07 22:00:51,380][flwr][INFO] - app_fit: losses_centralized [(0, 0.023086071193218233), (1, 0.021947802603244783), (2, 0.02080556857585907), (3, 0.019993782460689544)]
[2024-03-07 22:00:51,380][flwr][INFO] - app_fit: metrics_centralized {'accuracy': [(0, 27.63671875), (1, 28.30078125), (2, 45.21484375), (3, 48.76953125)]}
