[2024-03-07 22:03:24,891][flwr][WARNING] - Both server and strategy were provided, ignoring strategy
[2024-03-07 22:03:24,891][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
[2024-03-07 22:03:29,707][flwr][INFO] - Flower VCE: Ray initialized with resources: {'object_store_memory': 9679527936.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'memory': 19359055872.0, 'accelerator_type:G': 1.0, 'node:155.207.19.230': 1.0, 'GPU': 2.0}
[2024-03-07 22:03:29,707][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2024-03-07 22:03:29,707][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.5}
[2024-03-07 22:03:29,723][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
[2024-03-07 22:03:29,723][flwr][INFO] - Initializing global parameters
[2024-03-07 22:03:29,724][flwr][INFO] - Requesting initial parameters from one random client
[2024-03-07 22:03:33,614][flwr][INFO] - Received initial parameters from one random client
[2024-03-07 22:03:33,614][flwr][INFO] - Evaluating initial parameters
[2024-03-07 22:03:42,788][flwr][INFO] - initial parameters (loss, other metrics): 0.04577481746673584, {'accuracy': 26.171875}
[2024-03-07 22:03:42,789][flwr][INFO] - FL starting
[2024-03-07 22:03:42,789][flwr][DEBUG] - fit_round 1: strategy sampled 8 clients (out of 16)
[2024-03-07 22:03:59,964][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl
    x = self.layer2(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward
    out = self.bn2(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 55.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl\n    x = self.layer2(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward\n    out = self.bn2(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 55.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:03:59,964][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl
    x = self.layer2(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward
    out = self.bn2(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 55.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 1 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl\n    x = self.layer2(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward\n    out = self.bn2(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 55.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:04:00,774][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward
    out = self.bn1(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 51.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 8 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward\n    out = self.bn1(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 51.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:04:00,775][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward
    out = self.bn1(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 51.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 8 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward\n    out = self.bn1(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 51.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:04:04,223][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl
    x = self.layer2(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward
    out = self.bn2(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 55.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl\n    x = self.layer2(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward\n    out = self.bn2(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 55.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:04:04,224][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl
    x = self.layer2(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward
    out = self.bn2(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 55.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 0 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl\n    x = self.layer2(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward\n    out = self.bn2(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 55.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:04:04,894][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward
    out = self.bn1(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 51.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 2 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward\n    out = self.bn1(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 51.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:04:04,894][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward
    out = self.bn1(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 51.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 2 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward\n    out = self.bn1(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 51.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:04:06,851][flwr][DEBUG] - fit_round 1 received 4 results and 4 failures
[2024-03-07 22:04:13,728][flwr][INFO] - fit progress: (1, 0.033911979526281354, {'accuracy': 61.81640625}, 30.939328852982726)
[2024-03-07 22:04:13,729][flwr][DEBUG] - evaluate_round 1: strategy sampled 8 clients (out of 16)
[2024-03-07 22:04:20,902][flwr][DEBUG] - evaluate_round 1 received 8 results and 0 failures
[2024-03-07 22:04:20,903][flwr][DEBUG] - fit_round 2: strategy sampled 8 clients (out of 16)
[2024-03-07 22:04:25,391][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl
    x = self.layer2(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward
    out = self.bn2(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 47.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 11 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl\n    x = self.layer2(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward\n    out = self.bn2(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 47.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:04:25,392][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl
    x = self.layer2(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward
    out = self.bn2(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 47.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 11 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl\n    x = self.layer2(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward\n    out = self.bn2(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 47.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:04:26,423][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward
    out = self.bn1(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 53.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 15 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward\n    out = self.bn1(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 53.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:04:26,424][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward
    out = self.bn1(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 53.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 15 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward\n    out = self.bn1(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 53.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:04:30,495][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl
    x = self.layer2(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward
    out = self.bn2(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 47.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 14 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl\n    x = self.layer2(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward\n    out = self.bn2(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 47.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:04:30,495][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl
    x = self.layer2(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward
    out = self.bn2(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 47.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659550, ip=155.207.19.230, actor_id=21d5303977ec57226e9ed08101000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7f2bfc26ac80>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 14 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 274, in _forward_impl\n    x = self.layer2(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 97, in forward\n    out = self.bn2(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.75 GiB total capacity; 3.01 GiB already allocated; 47.50 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:04:30,798][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward
    out = self.bn1(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 53.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 12 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward\n    out = self.bn1(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 53.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)

[2024-03-07 22:04:30,798][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit
    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train
    outputs = net(images)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward
    x = self.resnet(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward
    return self._forward_impl(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl
    x = self.layer1(x)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward
    out = self.bn1(out)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 53.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=659544, ip=155.207.19.230, actor_id=ca12648dfaaafb90cee546a601000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x7ff4c633acb0>)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 12 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/client/numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n  File "/home/stavrosmpoul/codebaseMarch/client.py", line 86, in fit\n    train_loss, train_acc = train(self.model, self.trainloader, optim, epochs, self.device, mu)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 161, in train\n    outputs = net(images)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/codebaseMarch/model.py", line 117, in forward\n    x = self.resnet(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 285, in forward\n    return self._forward_impl(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 273, in _forward_impl\n    x = self.layer1(x)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/container.py", line 204, in forward\n    input = module(input)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torchvision/models/resnet.py", line 93, in forward\n    out = self.bn1(out)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward\n    return F.batch_norm(\n  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/functional.py", line 2450, in batch_norm\n    return torch.batch_norm(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.75 GiB total capacity; 2.14 GiB already allocated; 53.62 MiB free; 2.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n',)
[2024-03-07 22:04:33,883][flwr][DEBUG] - fit_round 2 received 4 results and 4 failures
