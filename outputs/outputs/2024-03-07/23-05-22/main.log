[2024-03-07 23:05:23,888][flwr][WARNING] - Both server and strategy were provided, ignoring strategy
[2024-03-07 23:05:23,888][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
[2024-03-07 23:05:28,746][flwr][INFO] - Flower VCE: Ray initialized with resources: {'object_store_memory': 7666634342.0, 'memory': 15333268686.0, 'CPU': 12.0, 'node:__internal_head__': 1.0, 'GPU': 2.0, 'node:155.207.19.230': 1.0, 'accelerator_type:G': 1.0}
[2024-03-07 23:05:28,746][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2024-03-07 23:05:28,746][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 1}
[2024-03-07 23:05:28,757][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[2024-03-07 23:05:28,757][flwr][INFO] - Initializing global parameters
[2024-03-07 23:05:28,757][flwr][INFO] - Requesting initial parameters from one random client
[2024-03-07 23:05:31,610][flwr][INFO] - Received initial parameters from one random client
[2024-03-07 23:05:31,784][flwr][INFO] - Evaluating initial parameters
[2024-03-07 23:05:33,074][flwr][ERROR] - CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 33.68 MiB already allocated; 3.62 MiB free; 44.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-03-07 23:05:33,075][flwr][ERROR] - Traceback (most recent call last):
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/server/app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/server/server.py", line 92, in fit
    res = self.strategy.evaluate(0, parameters=self.parameters)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/flwr/server/strategy/fedavg.py", line 165, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/home/stavrosmpoul/codebaseMarch/strategy.py", line 74, in evaluate_fn
    loss, accuracy = test(model, testloader, device)
  File "/home/stavrosmpoul/codebaseMarch/model.py", line 202, in test
    net.to(device)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/stavrosmpoul/miniconda3/envs/pyt_pg2/lib/python3.10/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.75 GiB total capacity; 33.68 MiB already allocated; 3.62 MiB free; 44.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

[2024-03-07 23:05:33,076][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 1, 'num_gpus': 1} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 1, 'num_gpus': 1}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
