[2024-02-27 13:46:14,849][flwr][WARNING] - Both server and strategy were provided, ignoring strategy
[2024-02-27 13:46:14,850][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)
[2024-02-27 13:46:21,745][flwr][INFO] - Flower VCE: Ray initialized with resources: {'object_store_memory': 437220556.0, 'node:127.0.0.1': 1.0, 'memory': 874441115.0, 'node:__internal_head__': 1.0, 'CPU': 4.0}
[2024-02-27 13:46:21,746][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2024-02-27 13:46:21,747][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.0}
[2024-02-27 13:46:21,775][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[2024-02-27 13:46:21,777][flwr][INFO] - Initializing global parameters
[2024-02-27 13:46:21,777][flwr][INFO] - Requesting initial parameters from one random client
[2024-02-27 13:46:23,234][flwr][ERROR] - Traceback (most recent call last):
  File "python\ray\_raylet.pyx", line 687, in ray._raylet.prepare_args_internal
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\serialization.py", line 468, in serialize
    return self._serialize_to_msgpack(value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\serialization.py", line 446, in _serialize_to_msgpack
    pickle5_serialized_object = self._serialize_to_pickle5(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\serialization.py", line 408, in _serialize_to_pickle5
    raise e
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\serialization.py", line 403, in _serialize_to_pickle5
    inband = pickle.dumps(
             ^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\cloudpickle\cloudpickle_fast.py", line 88, in dumps
    cp.dump(obj)
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\cloudpickle\cloudpickle_fast.py", line 733, in dump
    return Pickler.dump(self, obj)
           ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: cannot pickle 'torch._C.Generator' object

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 147, in _submit_job
    self.actor_pool.submit_client_job(
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 271, in submit_client_job
    self.submit(actor_fn, job)
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 249, in submit
    future = fn(actor, client_fn, job_fn, cid, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 148, in <lambda>
    lambda a, c_fn, j_fn, cid, state: a.run.remote(c_fn, j_fn, cid, state),
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 144, in remote
    return self._remote(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 423, in _start_span
    return method(self, args, kwargs, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 190, in _remote
    return invocation(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 177, in invocation
    return actor._actor_method_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 1175, in _actor_method_call
    object_refs = worker.core_worker.submit_actor_task(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "python\ray\_raylet.pyx", line 3350, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 3355, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 649, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 640, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 696, in ray._raylet.prepare_args_internal
TypeError: Could not serialize the argument <function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60> for a task or actor flwr.simulation.ray_transport.ray_actor.DefaultActor.run:
================================================================================
Checking Serializability of <function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60>
================================================================================
[31m!!! FAIL[39m serialization: cannot pickle 'torch._C.Generator' object
Detected 1 global variables. Checking serializability...
    Serializing 'FlowerClient' <class 'client.FlowerClient'>...
Detected 5 nonlocal variables. Checking serializability...
    Serializing 'epochs' 1...
    Serializing 'exp_config' {'num_rounds': 3, 'num_clients': 16, 'lr_scheduling': False, 'batch_size': 64, 'partitioning': 'dirichlet', 'alpha': 0.5, 'balance': True, 'num_classes': 4, 'subset': True, 'datapath': 'D:/DesktopC/Datasets/data/', 'num_workers': 1, 'num_clients_per_round_fit': 8, 'num_clients_per_round_eval': '${num_clients_per_round_fit}', 'seed': 2024, 'client_resources': {'num_cpus': 2, 'num_gpus': 0.0}, 'var_local_epochs': False, 'var_min_epochs': 1, 'var_max_epochs': 4, 'local_epochs': 1, 'model': {'_target_': 'model.ResNet18'}, 'strategy': {'name': 'fedavg', 'strategy': {'_target_': 'flwr.server.strategy.FedAvg', '_recursive_': False}, 'client_fn': {'_target_': 'client.generate_client_fn', '_recursive_': False}}, 'exp_name': 'momentum', 'optimizer': {'_target_': 'models.train', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0001, 'gmf': 0, 'mu': 0}}...
    Serializing 'num_classes' 4...
    Serializing 'trainloaders' [<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>]...
    [31m!!! FAIL[39m serialization: cannot pickle 'torch._C.Generator' object
    WARNING: Did not find non-serializable object in [<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>]. This may be an oversight.
================================================================================
Variable: 

	[1mFailTuple(trainloaders [obj=[<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>], parent=<function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60>])[0m

was found to be non-serializable. There may be multiple other undetected variables that were non-serializable. 
Consider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. 
================================================================================
Check https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.
If you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/
================================================================================


[2024-02-27 13:46:23,334][flwr][ERROR] - Could not serialize the argument <function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60> for a task or actor flwr.simulation.ray_transport.ray_actor.DefaultActor.run:
================================================================================
Checking Serializability of <function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60>
================================================================================
[31m!!! FAIL[39m serialization: cannot pickle 'torch._C.Generator' object
Detected 1 global variables. Checking serializability...
    Serializing 'FlowerClient' <class 'client.FlowerClient'>...
Detected 5 nonlocal variables. Checking serializability...
    Serializing 'epochs' 1...
    Serializing 'exp_config' {'num_rounds': 3, 'num_clients': 16, 'lr_scheduling': False, 'batch_size': 64, 'partitioning': 'dirichlet', 'alpha': 0.5, 'balance': True, 'num_classes': 4, 'subset': True, 'datapath': 'D:/DesktopC/Datasets/data/', 'num_workers': 1, 'num_clients_per_round_fit': 8, 'num_clients_per_round_eval': '${num_clients_per_round_fit}', 'seed': 2024, 'client_resources': {'num_cpus': 2, 'num_gpus': 0.0}, 'var_local_epochs': False, 'var_min_epochs': 1, 'var_max_epochs': 4, 'local_epochs': 1, 'model': {'_target_': 'model.ResNet18'}, 'strategy': {'name': 'fedavg', 'strategy': {'_target_': 'flwr.server.strategy.FedAvg', '_recursive_': False}, 'client_fn': {'_target_': 'client.generate_client_fn', '_recursive_': False}}, 'exp_name': 'momentum', 'optimizer': {'_target_': 'models.train', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0001, 'gmf': 0, 'mu': 0}}...
    Serializing 'num_classes' 4...
    Serializing 'trainloaders' [<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>]...
    [31m!!! FAIL[39m serialization: cannot pickle 'torch._C.Generator' object
    WARNING: Did not find non-serializable object in [<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>]. This may be an oversight.
================================================================================
Variable: 

	[1mFailTuple(trainloaders [obj=[<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>], parent=<function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60>])[0m

was found to be non-serializable. There may be multiple other undetected variables that were non-serializable. 
Consider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. 
================================================================================
Check https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.
If you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/
================================================================================

[2024-02-27 13:46:23,364][flwr][ERROR] - Could not serialize the argument <function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60> for a task or actor flwr.simulation.ray_transport.ray_actor.DefaultActor.run:
================================================================================
Checking Serializability of <function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60>
================================================================================
[31m!!! FAIL[39m serialization: cannot pickle 'torch._C.Generator' object
Detected 1 global variables. Checking serializability...
    Serializing 'FlowerClient' <class 'client.FlowerClient'>...
Detected 5 nonlocal variables. Checking serializability...
    Serializing 'epochs' 1...
    Serializing 'exp_config' {'num_rounds': 3, 'num_clients': 16, 'lr_scheduling': False, 'batch_size': 64, 'partitioning': 'dirichlet', 'alpha': 0.5, 'balance': True, 'num_classes': 4, 'subset': True, 'datapath': 'D:/DesktopC/Datasets/data/', 'num_workers': 1, 'num_clients_per_round_fit': 8, 'num_clients_per_round_eval': '${num_clients_per_round_fit}', 'seed': 2024, 'client_resources': {'num_cpus': 2, 'num_gpus': 0.0}, 'var_local_epochs': False, 'var_min_epochs': 1, 'var_max_epochs': 4, 'local_epochs': 1, 'model': {'_target_': 'model.ResNet18'}, 'strategy': {'name': 'fedavg', 'strategy': {'_target_': 'flwr.server.strategy.FedAvg', '_recursive_': False}, 'client_fn': {'_target_': 'client.generate_client_fn', '_recursive_': False}}, 'exp_name': 'momentum', 'optimizer': {'_target_': 'models.train', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0001, 'gmf': 0, 'mu': 0}}...
    Serializing 'num_classes' 4...
    Serializing 'trainloaders' [<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>]...
    [31m!!! FAIL[39m serialization: cannot pickle 'torch._C.Generator' object
    WARNING: Did not find non-serializable object in [<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>]. This may be an oversight.
================================================================================
Variable: 

	[1mFailTuple(trainloaders [obj=[<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>], parent=<function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60>])[0m

was found to be non-serializable. There may be multiple other undetected variables that were non-serializable. 
Consider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. 
================================================================================
Check https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.
If you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/
================================================================================

[2024-02-27 13:46:23,416][flwr][ERROR] - Traceback (most recent call last):
  File "python\ray\_raylet.pyx", line 687, in ray._raylet.prepare_args_internal
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\serialization.py", line 468, in serialize
    return self._serialize_to_msgpack(value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\serialization.py", line 446, in _serialize_to_msgpack
    pickle5_serialized_object = self._serialize_to_pickle5(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\serialization.py", line 408, in _serialize_to_pickle5
    raise e
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\_private\serialization.py", line 403, in _serialize_to_pickle5
    inband = pickle.dumps(
             ^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\cloudpickle\cloudpickle_fast.py", line 88, in dumps
    cp.dump(obj)
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\cloudpickle\cloudpickle_fast.py", line 733, in dump
    return Pickler.dump(self, obj)
           ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: cannot pickle 'torch._C.Generator' object

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\app.py", line 308, in start_simulation
    hist = run_fl(
           ^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\server\app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\server\server.py", line 90, in fit
    self.parameters = self._get_initial_parameters(timeout=timeout)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\server\server.py", line 279, in _get_initial_parameters
    get_parameters_res = random_client.get_parameters(ins=ins, timeout=timeout)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 196, in get_parameters
    res = self._submit_job(get_parameters, timeout)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 163, in _submit_job
    raise ex
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 147, in _submit_job
    self.actor_pool.submit_client_job(
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 271, in submit_client_job
    self.submit(actor_fn, job)
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 249, in submit
    future = fn(actor, client_fn, job_fn, cid, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 148, in <lambda>
    lambda a, c_fn, j_fn, cid, state: a.run.remote(c_fn, j_fn, cid, state),
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 144, in remote
    return self._remote(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 423, in _start_span
    return method(self, args, kwargs, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 190, in _remote
    return invocation(args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 177, in invocation
    return actor._actor_method_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Stavros\miniconda3\envs\pyt_pg\Lib\site-packages\ray\actor.py", line 1175, in _actor_method_call
    object_refs = worker.core_worker.submit_actor_task(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "python\ray\_raylet.pyx", line 3350, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 3355, in ray._raylet.CoreWorker.submit_actor_task
  File "python\ray\_raylet.pyx", line 649, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 640, in ray._raylet.prepare_args_and_increment_put_refs
  File "python\ray\_raylet.pyx", line 696, in ray._raylet.prepare_args_internal
TypeError: Could not serialize the argument <function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60> for a task or actor flwr.simulation.ray_transport.ray_actor.DefaultActor.run:
================================================================================
Checking Serializability of <function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60>
================================================================================
[31m!!! FAIL[39m serialization: cannot pickle 'torch._C.Generator' object
Detected 1 global variables. Checking serializability...
    Serializing 'FlowerClient' <class 'client.FlowerClient'>...
Detected 5 nonlocal variables. Checking serializability...
    Serializing 'epochs' 1...
    Serializing 'exp_config' {'num_rounds': 3, 'num_clients': 16, 'lr_scheduling': False, 'batch_size': 64, 'partitioning': 'dirichlet', 'alpha': 0.5, 'balance': True, 'num_classes': 4, 'subset': True, 'datapath': 'D:/DesktopC/Datasets/data/', 'num_workers': 1, 'num_clients_per_round_fit': 8, 'num_clients_per_round_eval': '${num_clients_per_round_fit}', 'seed': 2024, 'client_resources': {'num_cpus': 2, 'num_gpus': 0.0}, 'var_local_epochs': False, 'var_min_epochs': 1, 'var_max_epochs': 4, 'local_epochs': 1, 'model': {'_target_': 'model.ResNet18'}, 'strategy': {'name': 'fedavg', 'strategy': {'_target_': 'flwr.server.strategy.FedAvg', '_recursive_': False}, 'client_fn': {'_target_': 'client.generate_client_fn', '_recursive_': False}}, 'exp_name': 'momentum', 'optimizer': {'_target_': 'models.train', 'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0001, 'gmf': 0, 'mu': 0}}...
    Serializing 'num_classes' 4...
    Serializing 'trainloaders' [<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>]...
    [31m!!! FAIL[39m serialization: cannot pickle 'torch._C.Generator' object
    WARNING: Did not find non-serializable object in [<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>]. This may be an oversight.
================================================================================
Variable: 

	[1mFailTuple(trainloaders [obj=[<torch.utils.data.dataloader.DataLoader object at 0x000001A311F2FF10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312103E10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A3121C1390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311F73F90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FC5850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A311FD7BD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A310FA8C90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BCD0>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206BF90>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A610>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B390>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206A850>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069C10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A31206B810>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312069D10>, <torch.utils.data.dataloader.DataLoader object at 0x000001A312068390>], parent=<function generate_client_fn.<locals>.client_fn at 0x000001A3118F5C60>])[0m

was found to be non-serializable. There may be multiple other undetected variables that were non-serializable. 
Consider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. 
================================================================================
Check https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.
If you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/
================================================================================


[2024-02-27 13:46:23,444][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 0.0} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 0.0}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
