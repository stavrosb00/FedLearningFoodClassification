[2024-03-13 14:05:45,620][flwr][WARNING] - Both server and strategy were provided, ignoring strategy
[2024-03-13 14:05:45,621][flwr][INFO] - Starting Flower simulation, config: ServerConfig(num_rounds=1, round_timeout=None)
[2024-03-13 14:05:48,951][flwr][INFO] - Flower VCE: Ray initialized with resources: {'memory': 16065446708.0, 'object_store_memory': 8032723353.0, 'node:127.0.0.1': 1.0, 'node:__internal_head__': 1.0, 'CPU': 6.0, 'GPU': 1.0}
[2024-03-13 14:05:48,951][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
[2024-03-13 14:05:48,952][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 1}
[2024-03-13 14:05:48,961][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
[2024-03-13 14:05:48,961][flwr][INFO] - Initializing global parameters
[2024-03-13 14:05:48,962][flwr][INFO] - Requesting initial parameters from one random client
[2024-03-13 14:05:53,960][flwr][INFO] - Received initial parameters from one random client
[2024-03-13 14:05:53,978][flwr][INFO] - Evaluating initial parameters
[2024-03-13 14:06:07,836][flwr][INFO] - initial parameters (loss, other metrics): 0.023446575045585633, {'accuracy': 19.3359375}
[2024-03-13 14:06:07,837][flwr][INFO] - FL starting
[2024-03-13 14:06:07,897][flwr][DEBUG] - fit_round 1: strategy sampled 8 clients (out of 16)
[2024-03-13 14:06:20,272][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 2 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)

[2024-03-13 14:06:20,281][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 2 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)
[2024-03-13 14:06:33,345][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 11 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)

[2024-03-13 14:06:33,360][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 11 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)
[2024-03-13 14:06:48,449][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 3 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)

[2024-03-13 14:06:48,458][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 3 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)
[2024-03-13 14:07:03,534][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 13 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)

[2024-03-13 14:07:03,549][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 13 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)
[2024-03-13 14:07:18,616][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 4 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)

[2024-03-13 14:07:18,628][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 4 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)
[2024-03-13 14:07:33,521][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 6 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)

[2024-03-13 14:07:33,531][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 6 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)
[2024-03-13 14:07:48,860][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 15 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)

[2024-03-13 14:07:48,869][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 15 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)
[2024-03-13 14:08:03,519][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
                                    ^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientException): [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)

[2024-03-13 14:08:03,527][flwr][ERROR] - [36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
                  ^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_client_proxy.py", line 207, in fit
    return maybe_call_fit(
           ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\client\numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\client_scaffold.py", line 130, in fit
    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 268, in train_scaffold
    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\model.py", line 230, in step_custom
    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i' = - \eta*(c - c_i)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

The above exception was the direct cause of the following exception:

[36mray::DefaultActor.run()[39m (pid=15256, ip=127.0.0.1, actor_id=dc28007bfb5695d91fa2eaf501000000, repr=<flwr.simulation.ray_transport.ray_actor.DefaultActor object at 0x0000028BA4B06350>)
  File "python\ray\_raylet.pyx", line 1418, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1498, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1424, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1364, in ray._raylet.execute_task.function_executor
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 464, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 90, in run
    raise ClientException(str(message)) from ex
flwr.simulation.ray_transport.ray_actor.ClientException: 
>>>>>>>A ClientException occurred.('\n\tSomething went wrong when running your client run.\n\tClient 10 crashed when the DefaultActor was running its run.\n\tException triggered on the client side: Traceback (most recent call last):\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_actor.py", line 76, in run\n    job_results = job_fn(client)\n                  ^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\simulation\\ray_transport\\ray_client_proxy.py", line 207, in fit\n    return maybe_call_fit(\n           ^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\client.py", line 234, in maybe_call_fit\n    return client.fit(fit_ins)\n           ^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\miniconda3\\envs\\fresh_pyt\\Lib\\site-packages\\flwr\\client\\numpy_client.py", line 238, in _fit\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\client_scaffold.py", line 130, in fit\n    train_loss, train_acc = train_scaffold(self.model, self.trainloader, optim, epochs, self.device, server_cv, self.client_cv)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 268, in train_scaffold\n    optimizer.step_custom(server_cv=server_cv, client_cv=client_cv)\n  File "C:\\Users\\smpoulio_local\\Documents\\GitHub\\FlowerYtTut\\model.py", line 230, in step_custom\n    par.data.add_(s_cv - c_cv, alpha=-group["lr"]) # y_i\' = - \\eta*(c - c_i)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n',)
[2024-03-13 14:08:03,532][flwr][DEBUG] - fit_round 1 received 0 results and 8 failures
[2024-03-13 14:08:03,533][flwr][ERROR] - list index out of range
[2024-03-13 14:08:03,535][flwr][ERROR] - Traceback (most recent call last):
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\simulation\app.py", line 308, in start_simulation
    hist = run_fl(
           ^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\server\app.py", line 225, in run_fl
    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\miniconda3\envs\fresh_pyt\Lib\site-packages\flwr\server\server.py", line 109, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\server_scaffold.py", line 155, in fit_round
    self.server_cv = [
                     ^
  File "C:\Users\smpoulio_local\Documents\GitHub\FlowerYtTut\server_scaffold.py", line 156, in <listcomp>
    torch.from_numpy(cv + cv_multiplier * aggregated_cv_update[i])
                                          ~~~~~~~~~~~~~~~~~~~~^^^
IndexError: list index out of range

[2024-03-13 14:08:03,537][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 1, 'num_gpus': 1} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 1, 'num_gpus': 1}.
Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.
